{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Python 2年生 スクレイピングのしくみ (How scraping works)\n",
        "\n",
        "Chapter 2"
      ],
      "metadata": {
        "id": "ZPnnqqiDGjTN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lesson 04 HTMLを解析してみよう (Let's parse HTML)"
      ],
      "metadata": {
        "id": "1leMCpA6Ihas"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G173K8klGeY2",
        "outputId": "1d53fc9d-5cbd-47d5-e5d0-1ac0c38f3658"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!DOCTYPE html>\n",
            "\n",
            "<html>\n",
            "<head>\n",
            "<meta charset=\"utf-8\"/>\n",
            "<title>Python2年生</title>\n",
            "</head>\n",
            "<body>\n",
            "<h2>第1章 Pythonでデータをダウンロード</h2>\n",
            "<ol>\n",
            "<li>スクレイピングってなに？</li>\n",
            "<li>Pythonをインストールしてみよう</li>\n",
            "<li>requestsでアクセスしてみよう</li>\n",
            "</ol>\n",
            "</body>\n",
            "</html>\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Retrieve and analyze web pages\n",
        "load_url = \"https://www.ymori.com/books/python2nen/test1.html\"\n",
        "html = requests.get(load_url)\n",
        "soup = BeautifulSoup(html.content, \"html.parser\")\n",
        "\n",
        "# Display the entire HTML\n",
        "print(soup)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Search and display title, h2, li tags\n",
        "print(soup.find(\"title\"))\n",
        "print(soup.find(\"h2\"))\n",
        "print(soup.find(\"li\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYwyAK2OHE7h",
        "outputId": "c08c1758-4662-42ef-a04b-0590554f45a6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<title>Python2年生</title>\n",
            "<h2>第1章 Pythonでデータをダウンロード</h2>\n",
            "<li>スクレイピングってなに？</li>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Search for title, h2, li tags and display their strings\n",
        "print(soup.find(\"title\").text)\n",
        "print(soup.find(\"h2\").text)\n",
        "print(soup.find(\"li\").text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "465xpaoyHRpV",
        "outputId": "fd5ddd64-5489-426c-fefa-88e15e0e6b4c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python2年生\n",
            "第1章 Pythonでデータをダウンロード\n",
            "スクレイピングってなに？\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve and analyze web pages\n",
        "load_url = \"https://www.ymori.com/books/python2nen/test2.html\"\n",
        "html = requests.get(load_url)\n",
        "soup = BeautifulSoup(html.content, \"html.parser\")\n",
        "\n",
        "# Search for all li tags and display their strings\n",
        "for element in soup.find_all(\"li\"):\n",
        "    print(element.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebHrGgddHhS1",
        "outputId": "a4d45372-d425-4101-9157-af0c8f29ed95"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "スクレイピングってなに？\n",
            "Pythonをインストールしてみよう\n",
            "requestsでアクセスしてみよう\n",
            "HTMLを解析してみよう\n",
            "青空文庫の作品を取得してみよう\n",
            "リンク一覧をファイルに書き出そう\n",
            "画像を一括ダウンロードしよう\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Search by ID and display the contents of that tag\n",
        "chap2 = soup.find(id=\"chap2\")\n",
        "print(chap2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0VkXJO3HZn8",
        "outputId": "2a4cb2f0-c665-4d77-b3b1-89c2ddc0098f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<div id=\"chap2\">\n",
            "<h2>第2章 HTMLを解析しよう</h2>\n",
            "<ol>\n",
            "<li>HTMLを解析してみよう</li>\n",
            "<li>青空文庫の作品を取得してみよう</li>\n",
            "<li>リンク一覧をファイルに書き出そう</li>\n",
            "<li>画像を一括ダウンロードしよう</li>\n",
            "</ol>\n",
            "</div>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Search by ID and find and display all li tags in it\n",
        "for element in chap2.find_all(\"li\"):\n",
        "    print(element.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1PliI3BH_tf",
        "outputId": "8041a118-34dd-4305-cda8-b7d50fa39e58"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HTMLを解析してみよう\n",
            "青空文庫の作品を取得してみよう\n",
            "リンク一覧をファイルに書き出そう\n",
            "画像を一括ダウンロードしよう\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lesson 05 ニュースの最新記事一覧を取得してみよう (Let's get a list of the latest news articles)"
      ],
      "metadata": {
        "id": "ZJJ0nX6rIjf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Capture and analyze web pages\n",
        "load_url = \"https://www.nikkei-science.com/\"\n",
        "html = requests.get(load_url)\n",
        "soup = BeautifulSoup(html.content, \"html.parser\")\n",
        "\n",
        "# Search by class and find and display all a tags in it\n",
        "topic = soup.find(class_=\"table-whatsnew-area large\")\n",
        "for element in topic.find_all(\"a\"):\n",
        "    print(element.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCt9UfqXSnF0",
        "outputId": "1a0d5197-b25e-464e-92b3-911d568d2433"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t\t\t\t\t\t\t\t\tThe Hunt for Planet Nine / 発見なるか プラネット・ナイン\n",
            "英語で読む日経サイエンス\n",
            "\n",
            "\t\t\t\t\t\t\t\t\t金吸着スポンジ〜日経サイエンス2025年5月号より\n",
            "SCOPE ＆ ADVANCE\n",
            "\n",
            "\t\t\t\t\t\t\t\t\t光合成動物〜日経サイエンス2025年5月号より\n",
            "SCOPE ＆ ADVANCE\n",
            "\n",
            "\t\t\t\t\t\t\t\t\tThe Mystery of Matter / 電子の精密実験で迫る 消えた反物質の謎\n",
            "英語で読む日経サイエンス\n",
            "\n",
            "\t\t\t\t\t\t\t\t\t小惑星「ベンヌ」にも生命の材料〜日経サイエンス2025年5月号より\n",
            "SCOPE ＆ ADVANCE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lesson 06 リンク一覧をファイルに書き出そう (Export the list of links to a file)"
      ],
      "metadata": {
        "id": "ae0Y8rCrTQ2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "# Capture and analyze web pages\n",
        "load_url = \"https://www.ymori.com/books/python2nen/test2.html\"\n",
        "html = requests.get(load_url)\n",
        "soup = BeautifulSoup(html.content, \"html.parser\")\n",
        "\n",
        "# Search for all a tags and export links as absolute URLs\n",
        "for element in soup.find_all(\"a\"):\n",
        "    print(element.text)\n",
        "    url = element.get(\"href\")\n",
        "    link_url = urllib.parse.urljoin(load_url, url)\n",
        "    print(link_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrwtltXAiwno",
        "outputId": "c4704ad3-9ac1-4baa-eabc-a42d28a8f67f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "リンク1\n",
            "https://www.ymori.com/books/python2nen/test1.html\n",
            "リンク2\n",
            "https://www.ymori.com/books/python2nen/test3.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open a file in write mode\n",
        "filename = \"linklist.txt\"\n",
        "with open(filename, \"w\") as f:\n",
        "    # Search for all a tags and export links as absolute URLs\n",
        "    for element in soup.find_all(\"a\"):\n",
        "        url = element.get(\"href\")\n",
        "        link_url = urllib.parse.urljoin(load_url, url)\n",
        "        f.write(element.text+\"\\n\")\n",
        "        f.write(link_url+\"\\n\")\n",
        "        f.write(\"\\n\")"
      ],
      "metadata": {
        "id": "7yUKOX9SkZLX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lesson 07 画像を一括ダウンロードしよう (Let's download images in bulk)"
      ],
      "metadata": {
        "id": "QYvgmIV1lRns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Create a folder for saving\n",
        "out_folder = Path(\"download\")\n",
        "out_folder.mkdir(exist_ok=True)\n",
        "\n",
        "# Obtain image files\n",
        "image_url = \"https://www.ymori.com/books/python2nen/sample1.png\"\n",
        "imgdata = requests.get(image_url)\n",
        "\n",
        "# Extract the last file name from the URL and connect it to the name of the folder where the file is saved.\n",
        "filename = image_url.split(\"/\")[-1]\n",
        "out_path = out_folder.joinpath(filename)\n",
        "\n",
        "# Export image data to a file\n",
        "with open(out_path, mode=\"wb\") as f:\n",
        "    f.write(imgdata.content)"
      ],
      "metadata": {
        "id": "EIVLeZl8lglg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Search all img tags and retrieve links\n",
        "for element in soup.find_all(\"img\"):\n",
        "    src = element.get(\"src\")\n",
        "\n",
        "    # Absolute URL and display the file\n",
        "    image_url = urllib.parse.urljoin(load_url, src)\n",
        "    filename = image_url.split(\"/\")[-1]\n",
        "    print(image_url, \">>\", filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5lcDJe9nG9A",
        "outputId": "f5a94cc3-457f-471a-b415-23451a37c588"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.ymori.com/books/python2nen/sample1.png >> sample1.png\n",
            "https://www.ymori.com/books/python2nen/sample2.png >> sample2.png\n",
            "https://www.ymori.com/books/python2nen/sample3.png >> sample3.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# Create a folder for saving\n",
        "out_folder = Path(\"download2\")\n",
        "out_folder.mkdir(exist_ok=True)\n",
        "\n",
        "# Search all mg tags and retrieve links\n",
        "for element in soup.find_all(\"img\"):\n",
        "    src = element.get(\"src\")\n",
        "\n",
        "    # Create absolute URLs to retrieve image data\n",
        "    image_url = urllib.parse.urljoin(load_url, src)\n",
        "    imgdata = requests.get(image_url)\n",
        "\n",
        "    # Extract the last file name from the URL and connect it to the folder for storage.\n",
        "    filename = image_url.split(\"/\")[-1]\n",
        "    out_path = out_folder.joinpath(filename)\n",
        "\n",
        "    # Export image data to a file\n",
        "    with open(out_path, mode=\"wb\") as f:\n",
        "        f.write(imgdata.content)\n",
        "\n",
        "    # Accessed once, wait one second.\n",
        "    time.sleep(1)"
      ],
      "metadata": {
        "id": "ksFE_ml2nl3P"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}